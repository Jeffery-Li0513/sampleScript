{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('boston_housing_data.csv', header=None, index_col=None)\n",
    "x = data.loc[1:, 0:12]\n",
    "y = data.loc[1:, 13:13]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[24.0,\n",
       " 21.6,\n",
       " 34.7,\n",
       " 33.4,\n",
       " 36.2,\n",
       " 28.7,\n",
       " 22.9,\n",
       " 27.1,\n",
       " 16.5,\n",
       " 18.9,\n",
       " 15.0,\n",
       " 18.9,\n",
       " 21.7,\n",
       " 20.4,\n",
       " 18.2,\n",
       " 19.9,\n",
       " 23.1,\n",
       " 17.5,\n",
       " 20.2,\n",
       " 18.2,\n",
       " 13.6,\n",
       " 19.6,\n",
       " 15.2,\n",
       " 14.5,\n",
       " 15.6,\n",
       " 13.9,\n",
       " 16.6,\n",
       " 14.8,\n",
       " 18.4,\n",
       " 21.0,\n",
       " 12.7,\n",
       " 14.5,\n",
       " 13.2,\n",
       " 13.1,\n",
       " 13.5,\n",
       " 18.9,\n",
       " 20.0,\n",
       " 21.0,\n",
       " 24.7,\n",
       " 30.8,\n",
       " 34.9,\n",
       " 26.6,\n",
       " 25.3,\n",
       " 24.7,\n",
       " 21.2,\n",
       " 19.3,\n",
       " 20.0,\n",
       " 16.6,\n",
       " 14.4,\n",
       " 19.4,\n",
       " 19.7,\n",
       " 20.5,\n",
       " 25.0,\n",
       " 23.4,\n",
       " 18.9,\n",
       " 35.4,\n",
       " 24.7,\n",
       " 31.6,\n",
       " 23.3,\n",
       " 19.6,\n",
       " 18.7,\n",
       " 16.0,\n",
       " 22.2,\n",
       " 25.0,\n",
       " 33.0,\n",
       " 23.5,\n",
       " 19.4,\n",
       " 22.0,\n",
       " 17.4,\n",
       " 20.9,\n",
       " 24.2,\n",
       " 21.7,\n",
       " 22.8,\n",
       " 23.4,\n",
       " 24.1,\n",
       " 21.4,\n",
       " 20.0,\n",
       " 20.8,\n",
       " 21.2,\n",
       " 20.3,\n",
       " 28.0,\n",
       " 23.9,\n",
       " 24.8,\n",
       " 22.9,\n",
       " 23.9,\n",
       " 26.6,\n",
       " 22.5,\n",
       " 22.2,\n",
       " 23.6,\n",
       " 28.7,\n",
       " 22.6,\n",
       " 22.0,\n",
       " 22.9,\n",
       " 25.0,\n",
       " 20.6,\n",
       " 28.4,\n",
       " 21.4,\n",
       " 38.7,\n",
       " 43.8,\n",
       " 33.2,\n",
       " 27.5,\n",
       " 26.5,\n",
       " 18.6,\n",
       " 19.3,\n",
       " 20.1,\n",
       " 19.5,\n",
       " 19.5,\n",
       " 20.4,\n",
       " 19.8,\n",
       " 19.4,\n",
       " 21.7,\n",
       " 22.8,\n",
       " 18.8,\n",
       " 18.7,\n",
       " 18.5,\n",
       " 18.3,\n",
       " 21.2,\n",
       " 19.2,\n",
       " 20.4,\n",
       " 19.3,\n",
       " 22.0,\n",
       " 20.3,\n",
       " 20.5,\n",
       " 17.3,\n",
       " 18.8,\n",
       " 21.4,\n",
       " 15.7,\n",
       " 16.2,\n",
       " 18.0,\n",
       " 14.3,\n",
       " 19.2,\n",
       " 19.6,\n",
       " 23.0,\n",
       " 18.4,\n",
       " 15.6,\n",
       " 18.1,\n",
       " 17.4,\n",
       " 17.1,\n",
       " 13.3,\n",
       " 17.8,\n",
       " 14.0,\n",
       " 14.4,\n",
       " 13.4,\n",
       " 15.6,\n",
       " 11.8,\n",
       " 13.8,\n",
       " 15.6,\n",
       " 14.6,\n",
       " 17.8,\n",
       " 15.4,\n",
       " 21.5,\n",
       " 19.6,\n",
       " 15.3,\n",
       " 19.4,\n",
       " 17.0,\n",
       " 15.6,\n",
       " 13.1,\n",
       " 41.3,\n",
       " 24.3,\n",
       " 23.3,\n",
       " 27.0,\n",
       " 50.0,\n",
       " 50.0,\n",
       " 50.0,\n",
       " 22.7,\n",
       " 25.0,\n",
       " 50.0,\n",
       " 23.8,\n",
       " 23.8,\n",
       " 22.3,\n",
       " 17.4,\n",
       " 19.1,\n",
       " 23.1,\n",
       " 23.6,\n",
       " 22.6,\n",
       " 29.4,\n",
       " 23.2,\n",
       " 24.6,\n",
       " 29.9,\n",
       " 37.2,\n",
       " 39.8,\n",
       " 36.2,\n",
       " 37.9,\n",
       " 32.5,\n",
       " 26.4,\n",
       " 29.6,\n",
       " 50.0,\n",
       " 32.0,\n",
       " 29.8,\n",
       " 34.9,\n",
       " 37.0,\n",
       " 30.5,\n",
       " 36.4,\n",
       " 31.1,\n",
       " 29.1,\n",
       " 50.0,\n",
       " 33.3,\n",
       " 30.3,\n",
       " 34.6,\n",
       " 34.9,\n",
       " 32.9,\n",
       " 24.1,\n",
       " 42.3,\n",
       " 48.5,\n",
       " 50.0,\n",
       " 22.6,\n",
       " 24.4,\n",
       " 22.5,\n",
       " 24.4,\n",
       " 20.0,\n",
       " 21.7,\n",
       " 19.3,\n",
       " 22.4,\n",
       " 28.1,\n",
       " 23.7,\n",
       " 25.0,\n",
       " 23.3,\n",
       " 28.7,\n",
       " 21.5,\n",
       " 23.0,\n",
       " 26.7,\n",
       " 21.7,\n",
       " 27.5,\n",
       " 30.1,\n",
       " 44.8,\n",
       " 50.0,\n",
       " 37.6,\n",
       " 31.6,\n",
       " 46.7,\n",
       " 31.5,\n",
       " 24.3,\n",
       " 31.7,\n",
       " 41.7,\n",
       " 48.3,\n",
       " 29.0,\n",
       " 24.0,\n",
       " 25.1,\n",
       " 31.5,\n",
       " 23.7,\n",
       " 23.3,\n",
       " 22.0,\n",
       " 20.1,\n",
       " 22.2,\n",
       " 23.7,\n",
       " 17.6,\n",
       " 18.5,\n",
       " 24.3,\n",
       " 20.5,\n",
       " 24.5,\n",
       " 26.2,\n",
       " 24.4,\n",
       " 24.8,\n",
       " 29.6,\n",
       " 42.8,\n",
       " 21.9,\n",
       " 20.9,\n",
       " 44.0,\n",
       " 50.0,\n",
       " 36.0,\n",
       " 30.1,\n",
       " 33.8,\n",
       " 43.1,\n",
       " 48.8,\n",
       " 31.0,\n",
       " 36.5,\n",
       " 22.8,\n",
       " 30.7,\n",
       " 50.0,\n",
       " 43.5,\n",
       " 20.7,\n",
       " 21.1,\n",
       " 25.2,\n",
       " 24.4,\n",
       " 35.2,\n",
       " 32.4,\n",
       " 32.0,\n",
       " 33.2,\n",
       " 33.1,\n",
       " 29.1,\n",
       " 35.1,\n",
       " 45.4,\n",
       " 35.4,\n",
       " 46.0,\n",
       " 50.0,\n",
       " 32.2,\n",
       " 22.0,\n",
       " 20.1,\n",
       " 23.2,\n",
       " 22.3,\n",
       " 24.8,\n",
       " 28.5,\n",
       " 37.3,\n",
       " 27.9,\n",
       " 23.9,\n",
       " 21.7,\n",
       " 28.6,\n",
       " 27.1,\n",
       " 20.3,\n",
       " 22.5,\n",
       " 29.0,\n",
       " 24.8,\n",
       " 22.0,\n",
       " 26.4,\n",
       " 33.1,\n",
       " 36.1,\n",
       " 28.4,\n",
       " 33.4,\n",
       " 28.2,\n",
       " 22.8,\n",
       " 20.3,\n",
       " 16.1,\n",
       " 22.1,\n",
       " 19.4,\n",
       " 21.6,\n",
       " 23.8,\n",
       " 16.2,\n",
       " 17.8,\n",
       " 19.8,\n",
       " 23.1,\n",
       " 21.0,\n",
       " 23.8,\n",
       " 23.1,\n",
       " 20.4,\n",
       " 18.5,\n",
       " 25.0,\n",
       " 24.6,\n",
       " 23.0,\n",
       " 22.2,\n",
       " 19.3,\n",
       " 22.6,\n",
       " 19.8,\n",
       " 17.1,\n",
       " 19.4,\n",
       " 22.2,\n",
       " 20.7,\n",
       " 21.1,\n",
       " 19.5,\n",
       " 18.5,\n",
       " 20.6,\n",
       " 19.0,\n",
       " 18.7,\n",
       " 32.7,\n",
       " 16.5,\n",
       " 23.9,\n",
       " 31.2,\n",
       " 17.5,\n",
       " 17.2,\n",
       " 23.1,\n",
       " 24.5,\n",
       " 26.6,\n",
       " 22.9,\n",
       " 24.1,\n",
       " 18.6,\n",
       " 30.1,\n",
       " 18.2,\n",
       " 20.6,\n",
       " 17.8,\n",
       " 21.7,\n",
       " 22.7,\n",
       " 22.6,\n",
       " 25.0,\n",
       " 19.9,\n",
       " 20.8,\n",
       " 16.8,\n",
       " 21.9,\n",
       " 27.5,\n",
       " 21.9,\n",
       " nan,\n",
       " 50.0,\n",
       " 50.0,\n",
       " 50.0,\n",
       " 50.0,\n",
       " 50.0,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " 13.3,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " 11.3,\n",
       " 12.3,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " 11.5,\n",
       " 15.1,\n",
       " 23.2,\n",
       " nan,\n",
       " 13.8,\n",
       " nan,\n",
       " 13.1,\n",
       " 12.5,\n",
       " 8.5,\n",
       " nan,\n",
       " 6.3,\n",
       " nan,\n",
       " nan,\n",
       " 12.1,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " 17.2,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " 14.2,\n",
       " nan,\n",
       " 13.4,\n",
       " 11.7,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " 11.0,\n",
       " 9.5,\n",
       " 14.5,\n",
       " nan,\n",
       " 16.1,\n",
       " 14.3,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " 12.8,\n",
       " nan,\n",
       " 17.1,\n",
       " 18.4,\n",
       " 15.4,\n",
       " nan,\n",
       " nan,\n",
       " 14.9,\n",
       " 12.6,\n",
       " 14.1,\n",
       " 13.0,\n",
       " 13.4,\n",
       " 15.2,\n",
       " 16.1,\n",
       " 17.8,\n",
       " 14.9,\n",
       " 14.1,\n",
       " 12.7,\n",
       " 13.5,\n",
       " 14.9,\n",
       " 20.0,\n",
       " 16.4,\n",
       " 17.7,\n",
       " 19.5,\n",
       " 20.2,\n",
       " 21.4,\n",
       " 19.9,\n",
       " 19.0,\n",
       " 19.1,\n",
       " nan,\n",
       " nan,\n",
       " 19.9,\n",
       " 19.6,\n",
       " 23.2,\n",
       " 29.8,\n",
       " 13.8,\n",
       " 13.3,\n",
       " 16.7,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " 23.0,\n",
       " 23.7,\n",
       " 25.0,\n",
       " 21.8,\n",
       " 20.6,\n",
       " 21.2,\n",
       " 19.1,\n",
       " 20.6,\n",
       " 15.2,\n",
       " 7.0,\n",
       " 8.1,\n",
       " 13.6,\n",
       " 20.1,\n",
       " 21.8,\n",
       " 24.5,\n",
       " 23.1,\n",
       " 19.7,\n",
       " 18.3,\n",
       " 21.2,\n",
       " 17.5,\n",
       " 16.8,\n",
       " 22.4,\n",
       " 20.6,\n",
       " 23.9,\n",
       " 22.0,\n",
       " 11.9]"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(y[13].astype(float))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([24. , 21.6, 34.7, 33.4, 36.2, 28.7, 22.9, 27.1, 16.5, 18.9, 15. ,\n",
       "       18.9, 21.7, 20.4, 18.2, 19.9, 23.1, 17.5, 20.2, 18.2, 13.6, 19.6,\n",
       "       15.2, 14.5, 15.6, 13.9, 16.6, 14.8, 18.4, 21. , 12.7, 14.5, 13.2,\n",
       "       13.1, 13.5, 18.9, 20. , 21. , 24.7, 30.8, 34.9, 26.6, 25.3, 24.7,\n",
       "       21.2, 19.3, 20. , 16.6, 14.4, 19.4, 19.7, 20.5, 25. , 23.4, 18.9,\n",
       "       35.4, 24.7, 31.6, 23.3, 19.6, 18.7, 16. , 22.2, 25. , 33. , 23.5,\n",
       "       19.4, 22. , 17.4, 20.9, 24.2, 21.7, 22.8, 23.4, 24.1, 21.4, 20. ,\n",
       "       20.8, 21.2, 20.3, 28. , 23.9, 24.8, 22.9, 23.9, 26.6, 22.5, 22.2,\n",
       "       23.6, 28.7, 22.6, 22. , 22.9, 25. , 20.6, 28.4, 21.4, 38.7, 43.8,\n",
       "       33.2, 27.5, 26.5, 18.6, 19.3, 20.1, 19.5, 19.5, 20.4, 19.8, 19.4,\n",
       "       21.7, 22.8, 18.8, 18.7, 18.5, 18.3, 21.2, 19.2, 20.4, 19.3, 22. ,\n",
       "       20.3, 20.5, 17.3, 18.8, 21.4, 15.7, 16.2, 18. , 14.3, 19.2, 19.6,\n",
       "       23. , 18.4, 15.6, 18.1, 17.4, 17.1, 13.3, 17.8, 14. , 14.4, 13.4,\n",
       "       15.6, 11.8, 13.8, 15.6, 14.6, 17.8, 15.4, 21.5, 19.6, 15.3, 19.4,\n",
       "       17. , 15.6, 13.1, 41.3, 24.3, 23.3, 27. , 50. , 50. , 50. , 22.7,\n",
       "       25. , 50. , 23.8, 23.8, 22.3, 17.4, 19.1, 23.1, 23.6, 22.6, 29.4,\n",
       "       23.2, 24.6, 29.9, 37.2, 39.8, 36.2, 37.9, 32.5, 26.4, 29.6, 50. ,\n",
       "       32. , 29.8, 34.9, 37. , 30.5, 36.4, 31.1, 29.1, 50. , 33.3, 30.3,\n",
       "       34.6, 34.9, 32.9, 24.1, 42.3, 48.5, 50. , 22.6, 24.4, 22.5, 24.4,\n",
       "       20. , 21.7, 19.3, 22.4, 28.1, 23.7, 25. , 23.3, 28.7, 21.5, 23. ,\n",
       "       26.7, 21.7, 27.5, 30.1, 44.8, 50. , 37.6, 31.6, 46.7, 31.5, 24.3,\n",
       "       31.7, 41.7, 48.3, 29. , 24. , 25.1, 31.5, 23.7, 23.3, 22. , 20.1,\n",
       "       22.2, 23.7, 17.6, 18.5, 24.3, 20.5, 24.5, 26.2, 24.4, 24.8, 29.6,\n",
       "       42.8, 21.9, 20.9, 44. , 50. , 36. , 30.1, 33.8, 43.1, 48.8, 31. ,\n",
       "       36.5, 22.8, 30.7, 50. , 43.5, 20.7, 21.1, 25.2, 24.4, 35.2, 32.4,\n",
       "       32. , 33.2, 33.1, 29.1, 35.1, 45.4, 35.4, 46. , 50. , 32.2, 22. ,\n",
       "       20.1, 23.2, 22.3, 24.8, 28.5, 37.3, 27.9, 23.9, 21.7, 28.6, 27.1,\n",
       "       20.3, 22.5, 29. , 24.8, 22. , 26.4, 33.1, 36.1, 28.4, 33.4, 28.2,\n",
       "       22.8, 20.3, 16.1, 22.1, 19.4, 21.6, 23.8, 16.2, 17.8, 19.8, 23.1,\n",
       "       21. , 23.8, 23.1, 20.4, 18.5, 25. , 24.6, 23. , 22.2, 19.3, 22.6,\n",
       "       19.8, 17.1, 19.4, 22.2, 20.7, 21.1, 19.5, 18.5, 20.6, 19. , 18.7,\n",
       "       32.7, 16.5, 23.9, 31.2, 17.5, 17.2, 23.1, 24.5, 26.6, 22.9, 24.1,\n",
       "       18.6, 30.1, 18.2, 20.6, 17.8, 21.7, 22.7, 22.6, 25. , 19.9, 20.8,\n",
       "       16.8, 21.9, 27.5, 21.9,  nan, 50. , 50. , 50. , 50. , 50. ,  nan,\n",
       "        nan,  nan,  nan, 13.3,  nan,  nan,  nan,  nan, 11.3, 12.3,  nan,\n",
       "        nan,  nan,  nan,  nan, 11.5, 15.1, 23.2,  nan, 13.8,  nan, 13.1,\n",
       "       12.5,  8.5,  nan,  6.3,  nan,  nan, 12.1,  nan,  nan,  nan,  nan,\n",
       "        nan, 17.2,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,\n",
       "        nan,  nan,  nan, 14.2,  nan, 13.4, 11.7,  nan,  nan,  nan, 11. ,\n",
       "        9.5, 14.5,  nan, 16.1, 14.3,  nan,  nan,  nan,  nan,  nan, 12.8,\n",
       "        nan, 17.1, 18.4, 15.4,  nan,  nan, 14.9, 12.6, 14.1, 13. , 13.4,\n",
       "       15.2, 16.1, 17.8, 14.9, 14.1, 12.7, 13.5, 14.9, 20. , 16.4, 17.7,\n",
       "       19.5, 20.2, 21.4, 19.9, 19. , 19.1,  nan,  nan, 19.9, 19.6, 23.2,\n",
       "       29.8, 13.8, 13.3, 16.7,  nan,  nan,  nan, 23. , 23.7, 25. , 21.8,\n",
       "       20.6, 21.2, 19.1, 20.6, 15.2,  7. ,  8.1, 13.6, 20.1, 21.8, 24.5,\n",
       "       23.1, 19.7, 18.3, 21.2, 17.5, 16.8, 22.4, 20.6, 23.9, 22. , 11.9])"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array(list(y[13].astype(float)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure()\n",
    "plt.hist(np.array(list(y[13].astype(float))), bins=20) # hist方法来绘制直方图\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "scaler = StandardScaler(copy=True, with_mean=True, with_std=True)\n",
    "X = scaler.fit_transform(x)\n",
    "Y = scaler.fit_transform(y)\n",
    "\n",
    "# from sklearn.preprocessing import MinMaxScaler # 导入归一化模块\n",
    " \n",
    "# # feature_range控制压缩数据范围，默认[0,1]\n",
    "# scaler = MinMaxScaler(feature_range=[0,1]) # 实例化，调整0,1的数值可以改变归一化范围\n",
    " \n",
    "# X = scaler.fit_transform(x)  # 将标签归一化到0,1之间\n",
    "# Y = scaler.fit_transform(y)  # 将特征归于化到0,1之间\n",
    " \n",
    "# # x = scaler.inverse_transform(X) # 将数据恢复至归一化之前"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\zefengli\\AppData\\Local\\Temp\\ipykernel_49840\\1920860409.py:3: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  X = torch.tensor(X, dtype=torch.float32)\n",
      "C:\\Users\\zefengli\\AppData\\Local\\Temp\\ipykernel_49840\\1920860409.py:4: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  Y = torch.tensor(Y, dtype=torch.float32)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "X = torch.tensor(X, dtype=torch.float32)\n",
    "Y = torch.tensor(Y, dtype=torch.float32)\n",
    "torch_dataset = torch.utils.data.TensorDataset(X, Y)\n",
    "batch_size = 128\n",
    "\n",
    "# 划分训练集测试集与验证集\n",
    "torch.manual_seed(2023)\n",
    "train_validation, test = torch.utils.data.random_split(torch_dataset, [450, 56])            # 先将数据集拆分为训练集+验证集（共450组），测试集（56组）\n",
    "train, validation = torch.utils.data.random_split(train_validation, [400, 50])            # 再将训练集+验证集拆分为训练集（400组），验证集（50组）\n",
    "train_data = torch.utils.data.DataLoader(train, batch_size=batch_size, shuffle=True)      # 再将训练集划分批次，每batch_size个数据一批（测试集与验证集不划分批次）"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "神经网络的计算可以分为输入数据的正向传播过程、误差的反向传播过程、权重及偏差矩阵的更新过程。\n",
    "\n",
    "正向传播过程：数据从输入层出来后需要先进行非线性激活，再进入隐藏层；从一个隐藏层出来之后，需要先非线性激活，再到进入下一个隐藏层；经过了所有隐藏层之后，再进入输出层；输出层输出数据后使用误差函数计算预测值与实际值的误差。\n",
    "\n",
    "反向传播过程：反向传播过程与正向传播过程相反，而且也不是矩阵求积运算，而是基于链式法则的求偏导运算（ 误差对偏差矩阵以及误差对权重矩阵的偏导数 ）。\n",
    "\n",
    "更新权重及偏差矩阵：根据反向传播过程计算出的偏导数更新偏差矩阵以及权重矩阵，最终使得偏差最小或者达到结束条件后停止计算。"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "考虑输入层的释放神经元数目、输出层的接收神经元数目、隐藏层接收和释放神经元数目以及隐藏层数的选择，还有激活函数、误差计算函数以及权重和偏差更新规则的选择"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\zefengli\\AppData\\Roaming\\Python\\Python39\\site-packages\\sklearn\\utils\\deprecation.py:87: FutureWarning: Function load_boston is deprecated; `load_boston` is deprecated in 1.0 and will be removed in 1.2.\n",
      "\n",
      "    The Boston housing prices dataset has an ethical problem. You can refer to\n",
      "    the documentation of this function for further details.\n",
      "\n",
      "    The scikit-learn maintainers therefore strongly discourage the use of this\n",
      "    dataset unless the purpose of the code is to study and educate about\n",
      "    ethical issues in data science and machine learning.\n",
      "\n",
      "    In this special case, you can fetch the dataset from the original\n",
      "    source::\n",
      "\n",
      "        import pandas as pd\n",
      "        import numpy as np\n",
      "\n",
      "        data_url = \"http://lib.stat.cmu.edu/datasets/boston\"\n",
      "        raw_df = pd.read_csv(data_url, sep=\"\\s+\", skiprows=22, header=None)\n",
      "        data = np.hstack([raw_df.values[::2, :], raw_df.values[1::2, :2]])\n",
      "        target = raw_df.values[1::2, 2]\n",
      "\n",
      "    Alternative datasets include the California housing dataset (i.e.\n",
      "    :func:`~sklearn.datasets.fetch_california_housing`) and the Ames housing\n",
      "    dataset. You can load the datasets as follows::\n",
      "\n",
      "        from sklearn.datasets import fetch_california_housing\n",
      "        housing = fetch_california_housing()\n",
      "\n",
      "    for the California housing dataset and::\n",
      "\n",
      "        from sklearn.datasets import fetch_openml\n",
      "        housing = fetch_openml(name=\"house_prices\", as_frame=True)\n",
      "\n",
      "    for the Ames housing dataset.\n",
      "  warnings.warn(msg, category=FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "# 是因为数据集的问题？\n",
    "\n",
    "from sklearn.datasets import load_boston\n",
    "import torch.utils.data as Data\n",
    "# 读入数据\n",
    "boston_X, boston_y = load_boston(return_X_y=True)\n",
    "ss = StandardScaler(copy=True, with_mean=True, with_std=True)\n",
    "# boston_Xs = ss.fit_transform(boston_X)\n",
    "boston_Xs = ss.fit_transform(pd.DataFrame(boston_X))\n",
    "# 将数据转化为张量\n",
    "train_xt = torch.from_numpy(boston_Xs.astype(np.float32))\n",
    "train_yt = torch.from_numpy(boston_y.astype(np.float32))\n",
    "# train_data = Data.TensorDataset(train_xt, train_yt)\n",
    "torch_dataset = Data.TensorDataset(train_xt, train_yt)\n",
    "# train_loader = Data.DataLoader(dataset=train_data,\n",
    "#                                batch_size=128,\n",
    "#                                shuffle=False,\n",
    "#                                num_workers=0)\n",
    "# 划分训练集测试集与验证集\n",
    "torch.manual_seed(2023)\n",
    "train_validation, test = torch.utils.data.random_split(torch_dataset, [450, 56])            # 先将数据集拆分为训练集+验证集（共450组），测试集（56组）\n",
    "train, validation = torch.utils.data.random_split(train_validation, [400, 50])            # 再将训练集+验证集拆分为训练集（400组），验证集（50组）\n",
    "train_data = torch.utils.data.DataLoader(train, batch_size=batch_size, shuffle=True)      # 再将训练集划分批次，每batch_size个数据一批（测试集与验证集不划分批次）\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 这个数据集有问题\n",
    "\n",
    "# data = pd.read_csv('boston_housing_data.csv', header=None, index_col=None)\n",
    "# x = data.loc[1:, 0:12]\n",
    "# y = data.loc[1:, 13:13]\n",
    "# x = x.astype(float)\n",
    "# y = y.astype(float)\n",
    "# from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "# scaler = StandardScaler(copy=True, with_mean=True, with_std=True)\n",
    "# X = scaler.fit_transform(x)\n",
    "# # Y = scaler.fit_transform(y)\n",
    "# train_x = torch.from_numpy(X.astype(np.float32))\n",
    "# train_y = torch.from_numpy(np.array(list(y[13].astype(float))).astype(np.float32))\n",
    "# X = torch.tensor(X, dtype=torch.float32)\n",
    "# Y = torch.tensor(Y, dtype=torch.float32)\n",
    "# # torch_dataset = torch.utils.data.TensorDataset(X, Y)\n",
    "# torch_dataset = torch.utils.data.TensorDataset(train_x, train_y)\n",
    "# batch_size = 128\n",
    "# train_dataset = torch.utils.data.DataLoader(torch_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "# # 划分训练集测试集与验证集\n",
    "# torch.manual_seed(2023)\n",
    "# train_validation, test = torch.utils.data.random_split(torch_dataset, [450, 56])            # 先将数据集拆分为训练集+验证集（共450组），测试集（56组）\n",
    "# train, validation = torch.utils.data.random_split(train_validation, [400, 50])            # 再将训练集+验证集拆分为训练集（400组），验证集（50组）\n",
    "# train_data = torch.utils.data.DataLoader(train, batch_size=batch_size, shuffle=True)      # 再将训练集划分批次，每batch_size个数据一批（测试集与验证集不划分批次）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train epoch 1, loss 576.4336547851562:\n",
      "epoch: 0, loss: 584.6564331054688\n",
      "train epoch 2, loss 401.9476318359375:\n",
      "epoch: 1, loss: 488.3126525878906\n",
      "train epoch 3, loss 287.6297912597656:\n",
      "epoch: 2, loss: 181.47509765625\n",
      "train epoch 4, loss 161.314208984375:\n",
      "epoch: 3, loss: 305.01800537109375\n",
      "train epoch 5, loss 60.604644775390625:\n",
      "epoch: 4, loss: 66.53588104248047\n",
      "train epoch 6, loss 50.486324310302734:\n",
      "epoch: 5, loss: 106.45527648925781\n",
      "train epoch 7, loss 41.58079528808594:\n",
      "epoch: 6, loss: 47.0567741394043\n",
      "train epoch 8, loss 29.49065399169922:\n",
      "epoch: 7, loss: 51.636512756347656\n",
      "train epoch 9, loss 35.3763542175293:\n",
      "epoch: 8, loss: 31.882667541503906\n",
      "train epoch 10, loss 33.40911102294922:\n",
      "epoch: 9, loss: 27.655038833618164\n",
      "train epoch 11, loss 21.427806854248047:\n",
      "epoch: 10, loss: 22.6124210357666\n",
      "train epoch 12, loss 17.12319564819336:\n",
      "epoch: 11, loss: 28.9333438873291\n",
      "train epoch 13, loss 51.52842712402344:\n",
      "epoch: 12, loss: 26.98272705078125\n",
      "train epoch 14, loss 30.797908782958984:\n",
      "epoch: 13, loss: 22.32083511352539\n",
      "train epoch 15, loss 50.75824737548828:\n",
      "epoch: 14, loss: 26.931440353393555\n",
      "train epoch 16, loss 13.051828384399414:\n",
      "epoch: 15, loss: 34.84563064575195\n",
      "train epoch 17, loss 44.11403274536133:\n",
      "epoch: 16, loss: 27.03289222717285\n",
      "train epoch 18, loss 8.959372520446777:\n",
      "epoch: 17, loss: 26.9670352935791\n",
      "train epoch 19, loss 14.025568008422852:\n",
      "epoch: 18, loss: 26.490833282470703\n",
      "train epoch 20, loss 35.860572814941406:\n",
      "epoch: 19, loss: 25.893211364746094\n",
      "train epoch 21, loss 13.179374694824219:\n",
      "epoch: 20, loss: 27.34314727783203\n",
      "train epoch 22, loss 12.420698165893555:\n",
      "epoch: 21, loss: 22.243114471435547\n",
      "train epoch 23, loss 31.635482788085938:\n",
      "epoch: 22, loss: 20.695466995239258\n",
      "train epoch 24, loss 12.969808578491211:\n",
      "epoch: 23, loss: 23.936073303222656\n",
      "train epoch 25, loss 25.97992706298828:\n",
      "epoch: 24, loss: 21.35643768310547\n",
      "train epoch 26, loss 9.825986862182617:\n",
      "epoch: 25, loss: 23.803518295288086\n",
      "train epoch 27, loss 12.939433097839355:\n",
      "epoch: 26, loss: 26.618112564086914\n",
      "train epoch 28, loss 10.871639251708984:\n",
      "epoch: 27, loss: 24.232927322387695\n",
      "train epoch 29, loss 3.9154014587402344:\n",
      "epoch: 28, loss: 23.98134994506836\n",
      "train epoch 30, loss 11.64694595336914:\n",
      "epoch: 29, loss: 24.9228515625\n",
      "train epoch 31, loss 7.1827073097229:\n",
      "epoch: 30, loss: 22.143983840942383\n",
      "train epoch 32, loss 7.336573600769043:\n",
      "epoch: 31, loss: 21.333740234375\n",
      "train epoch 33, loss 11.302886962890625:\n",
      "epoch: 32, loss: 22.755205154418945\n",
      "train epoch 34, loss 10.298270225524902:\n",
      "epoch: 33, loss: 21.48229217529297\n",
      "train epoch 35, loss 14.193887710571289:\n",
      "epoch: 34, loss: 24.261951446533203\n",
      "train epoch 36, loss 4.989194393157959:\n",
      "epoch: 35, loss: 26.128681182861328\n",
      "train epoch 37, loss 4.996866703033447:\n",
      "epoch: 36, loss: 21.9917049407959\n",
      "train epoch 38, loss 3.5520057678222656:\n",
      "epoch: 37, loss: 23.166372299194336\n",
      "train epoch 39, loss 3.7008559703826904:\n",
      "epoch: 38, loss: 25.820775985717773\n",
      "train epoch 40, loss 6.092274188995361:\n",
      "epoch: 39, loss: 23.662033081054688\n",
      "train epoch 41, loss 4.702552795410156:\n",
      "epoch: 40, loss: 24.363037109375\n",
      "train epoch 42, loss 12.758590698242188:\n",
      "epoch: 41, loss: 23.76650619506836\n",
      "train epoch 43, loss 9.387103080749512:\n",
      "epoch: 42, loss: 21.105628967285156\n",
      "train epoch 44, loss 1.981162667274475:\n",
      "epoch: 43, loss: 29.594192504882812\n",
      "train epoch 45, loss 18.168926239013672:\n",
      "epoch: 44, loss: 24.870573043823242\n",
      "train epoch 46, loss 4.037515640258789:\n",
      "epoch: 45, loss: 23.798877716064453\n",
      "train epoch 47, loss 20.13907241821289:\n",
      "epoch: 46, loss: 28.13577651977539\n",
      "train epoch 48, loss 2.758352279663086:\n",
      "epoch: 47, loss: 26.712820053100586\n",
      "train epoch 49, loss 14.974952697753906:\n",
      "epoch: 48, loss: 25.19119644165039\n",
      "train epoch 50, loss 5.8242340087890625:\n",
      "epoch: 49, loss: 28.459068298339844\n",
      "train epoch 51, loss 12.108333587646484:\n",
      "epoch: 50, loss: 24.05682373046875\n",
      "train epoch 52, loss 7.099130630493164:\n",
      "epoch: 51, loss: 25.394853591918945\n",
      "train epoch 53, loss 6.659172058105469:\n",
      "epoch: 52, loss: 23.712011337280273\n",
      "train epoch 54, loss 5.759370803833008:\n",
      "epoch: 53, loss: 25.230966567993164\n",
      "train epoch 55, loss 4.512204647064209:\n",
      "epoch: 54, loss: 25.0416316986084\n",
      "train epoch 56, loss 16.832916259765625:\n",
      "epoch: 55, loss: 23.214252471923828\n",
      "train epoch 57, loss 5.586509704589844:\n",
      "epoch: 56, loss: 30.41583251953125\n",
      "train epoch 58, loss 4.898159503936768:\n",
      "epoch: 57, loss: 26.593421936035156\n",
      "train epoch 59, loss 4.860824108123779:\n",
      "epoch: 58, loss: 26.573497772216797\n",
      "train epoch 60, loss 6.156364917755127:\n",
      "epoch: 59, loss: 24.05465316772461\n",
      "train epoch 61, loss 6.805851936340332:\n",
      "epoch: 60, loss: 23.794267654418945\n",
      "train epoch 62, loss 1.5347259044647217:\n",
      "epoch: 61, loss: 25.370534896850586\n",
      "train epoch 63, loss 2.1323328018188477:\n",
      "epoch: 62, loss: 25.423425674438477\n",
      "train epoch 64, loss 5.139888286590576:\n",
      "epoch: 63, loss: 27.613361358642578\n",
      "train epoch 65, loss 3.7453999519348145:\n",
      "epoch: 64, loss: 27.795869827270508\n",
      "train epoch 66, loss 12.264935493469238:\n",
      "epoch: 65, loss: 25.3679256439209\n",
      "train epoch 67, loss 5.046647071838379:\n",
      "epoch: 66, loss: 25.337387084960938\n",
      "train epoch 68, loss 6.98954963684082:\n",
      "epoch: 67, loss: 24.86798667907715\n",
      "train epoch 69, loss 11.975894927978516:\n",
      "epoch: 68, loss: 26.316225051879883\n",
      "train epoch 70, loss 10.641132354736328:\n",
      "epoch: 69, loss: 30.061012268066406\n",
      "train epoch 71, loss 7.579160213470459:\n",
      "epoch: 70, loss: 28.661781311035156\n",
      "train epoch 72, loss 2.6128010749816895:\n",
      "epoch: 71, loss: 27.789813995361328\n",
      "train epoch 73, loss 7.468921184539795:\n",
      "epoch: 72, loss: 24.392541885375977\n",
      "train epoch 74, loss 4.380294322967529:\n",
      "epoch: 73, loss: 28.461610794067383\n",
      "train epoch 75, loss 3.7390997409820557:\n",
      "epoch: 74, loss: 27.315813064575195\n",
      "train epoch 76, loss 7.203850269317627:\n",
      "epoch: 75, loss: 23.43254852294922\n",
      "train epoch 77, loss 3.664663314819336:\n",
      "epoch: 76, loss: 25.71611785888672\n",
      "train epoch 78, loss 10.529008865356445:\n",
      "epoch: 77, loss: 25.073135375976562\n",
      "train epoch 79, loss 3.583648204803467:\n",
      "epoch: 78, loss: 25.559309005737305\n",
      "train epoch 80, loss 10.839631080627441:\n",
      "epoch: 79, loss: 24.810152053833008\n",
      "train epoch 81, loss 3.978729248046875:\n",
      "epoch: 80, loss: 22.160987854003906\n",
      "train epoch 82, loss 4.983700275421143:\n",
      "epoch: 81, loss: 26.781631469726562\n",
      "train epoch 83, loss 3.593036651611328:\n",
      "epoch: 82, loss: 24.326250076293945\n",
      "train epoch 84, loss 12.720880508422852:\n",
      "epoch: 83, loss: 30.096656799316406\n",
      "train epoch 85, loss 8.161032676696777:\n",
      "epoch: 84, loss: 25.044334411621094\n",
      "train epoch 86, loss 11.683409690856934:\n",
      "epoch: 85, loss: 21.352670669555664\n",
      "train epoch 87, loss 3.8677260875701904:\n",
      "epoch: 86, loss: 26.83753204345703\n",
      "train epoch 88, loss 3.170196056365967:\n",
      "epoch: 87, loss: 22.42276382446289\n",
      "train epoch 89, loss 4.148927211761475:\n",
      "epoch: 88, loss: 26.860595703125\n",
      "train epoch 90, loss 9.260581016540527:\n",
      "epoch: 89, loss: 23.892290115356445\n",
      "train epoch 91, loss 4.8107075691223145:\n",
      "epoch: 90, loss: 23.486181259155273\n",
      "train epoch 92, loss 6.710339069366455:\n",
      "epoch: 91, loss: 25.1119441986084\n",
      "train epoch 93, loss 8.156811714172363:\n",
      "epoch: 92, loss: 25.297231674194336\n",
      "train epoch 94, loss 20.045780181884766:\n",
      "epoch: 93, loss: 32.807899475097656\n",
      "train epoch 95, loss 11.024293899536133:\n",
      "epoch: 94, loss: 28.935819625854492\n",
      "train epoch 96, loss 7.991805553436279:\n",
      "epoch: 95, loss: 26.214153289794922\n",
      "train epoch 97, loss 3.3165225982666016:\n",
      "epoch: 96, loss: 29.7907772064209\n",
      "train epoch 98, loss 3.386977434158325:\n",
      "epoch: 97, loss: 25.347694396972656\n",
      "train epoch 99, loss 1.4518780708312988:\n",
      "epoch: 98, loss: 28.79470443725586\n",
      "train epoch 100, loss 7.584671497344971:\n",
      "epoch: 99, loss: 25.572490692138672\n",
      "train epoch 101, loss 2.976102352142334:\n",
      "epoch: 100, loss: 28.177440643310547\n",
      "train epoch 102, loss 2.9968061447143555:\n",
      "epoch: 101, loss: 30.468992233276367\n",
      "train epoch 103, loss 6.7122416496276855:\n",
      "epoch: 102, loss: 25.042461395263672\n",
      "train epoch 104, loss 2.7467172145843506:\n",
      "epoch: 103, loss: 30.24896812438965\n",
      "train epoch 105, loss 4.024395942687988:\n",
      "epoch: 104, loss: 27.488414764404297\n",
      "train epoch 106, loss 4.82044792175293:\n",
      "epoch: 105, loss: 27.706613540649414\n",
      "train epoch 107, loss 2.8244504928588867:\n",
      "epoch: 106, loss: 30.498674392700195\n",
      "train epoch 108, loss 5.935571670532227:\n",
      "epoch: 107, loss: 28.889089584350586\n",
      "train epoch 109, loss 3.5452301502227783:\n",
      "epoch: 108, loss: 30.264657974243164\n",
      "train epoch 110, loss 3.8260385990142822:\n",
      "epoch: 109, loss: 26.780317306518555\n",
      "train epoch 111, loss 4.118765830993652:\n",
      "epoch: 110, loss: 26.840991973876953\n",
      "train epoch 112, loss 2.9172821044921875:\n",
      "epoch: 111, loss: 31.510292053222656\n",
      "train epoch 113, loss 4.676101207733154:\n",
      "epoch: 112, loss: 27.330036163330078\n",
      "train epoch 114, loss 3.37437105178833:\n",
      "epoch: 113, loss: 25.71601104736328\n",
      "train epoch 115, loss 10.125781059265137:\n",
      "epoch: 114, loss: 27.989721298217773\n",
      "train epoch 116, loss 2.625654458999634:\n",
      "epoch: 115, loss: 27.055147171020508\n",
      "train epoch 117, loss 3.6798698902130127:\n",
      "epoch: 116, loss: 27.439504623413086\n",
      "train epoch 118, loss 2.2074806690216064:\n",
      "epoch: 117, loss: 29.039743423461914\n",
      "train epoch 119, loss 6.954010963439941:\n",
      "epoch: 118, loss: 32.191261291503906\n",
      "train epoch 120, loss 10.545600891113281:\n",
      "epoch: 119, loss: 30.107702255249023\n",
      "train epoch 121, loss 5.784615516662598:\n",
      "epoch: 120, loss: 25.331571578979492\n",
      "train epoch 122, loss 2.6747965812683105:\n",
      "epoch: 121, loss: 25.550464630126953\n",
      "train epoch 123, loss 4.31139612197876:\n",
      "epoch: 122, loss: 26.88555335998535\n",
      "train epoch 124, loss 7.784321308135986:\n",
      "epoch: 123, loss: 29.238080978393555\n",
      "train epoch 125, loss 6.658509731292725:\n",
      "epoch: 124, loss: 28.53815269470215\n",
      "train epoch 126, loss 3.537024736404419:\n",
      "epoch: 125, loss: 31.710952758789062\n",
      "train epoch 127, loss 6.9694318771362305:\n",
      "epoch: 126, loss: 28.880508422851562\n",
      "train epoch 128, loss 4.189504623413086:\n",
      "epoch: 127, loss: 27.385465621948242\n",
      "train epoch 129, loss 4.453011512756348:\n",
      "epoch: 128, loss: 27.846712112426758\n",
      "train epoch 130, loss 8.789466857910156:\n",
      "epoch: 129, loss: 32.659175872802734\n",
      "train epoch 131, loss 7.550961971282959:\n",
      "epoch: 130, loss: 30.24451446533203\n",
      "train epoch 132, loss 6.508042335510254:\n",
      "epoch: 131, loss: 27.433334350585938\n",
      "train epoch 133, loss 4.129525184631348:\n",
      "epoch: 132, loss: 27.4968204498291\n",
      "train epoch 134, loss 12.25914478302002:\n",
      "epoch: 133, loss: 27.52713394165039\n",
      "train epoch 135, loss 2.153568983078003:\n",
      "epoch: 134, loss: 28.50446128845215\n",
      "train epoch 136, loss 3.9878039360046387:\n",
      "epoch: 135, loss: 27.900487899780273\n",
      "train epoch 137, loss 13.009527206420898:\n",
      "epoch: 136, loss: 32.93320846557617\n",
      "train epoch 138, loss 6.488698959350586:\n",
      "epoch: 137, loss: 30.477073669433594\n",
      "train epoch 139, loss 6.526905059814453:\n",
      "epoch: 138, loss: 28.0880069732666\n",
      "train epoch 140, loss 6.666412830352783:\n",
      "epoch: 139, loss: 31.432100296020508\n",
      "train epoch 141, loss 2.9754793643951416:\n",
      "epoch: 140, loss: 28.7678165435791\n",
      "train epoch 142, loss 3.9239232540130615:\n",
      "epoch: 141, loss: 30.301681518554688\n",
      "train epoch 143, loss 6.892615795135498:\n",
      "epoch: 142, loss: 33.383949279785156\n",
      "train epoch 144, loss 4.441415786743164:\n",
      "epoch: 143, loss: 28.236108779907227\n",
      "train epoch 145, loss 3.6866989135742188:\n",
      "epoch: 144, loss: 29.699052810668945\n",
      "train epoch 146, loss 3.9629580974578857:\n",
      "epoch: 145, loss: 28.534597396850586\n",
      "train epoch 147, loss 2.079559803009033:\n",
      "epoch: 146, loss: 29.2156925201416\n",
      "train epoch 148, loss 2.5121092796325684:\n",
      "epoch: 147, loss: 26.756986618041992\n",
      "train epoch 149, loss 5.185909271240234:\n",
      "epoch: 148, loss: 28.66763687133789\n",
      "train epoch 150, loss 3.083383083343506:\n",
      "epoch: 149, loss: 29.13092803955078\n",
      "train epoch 151, loss 2.955862522125244:\n",
      "epoch: 150, loss: 33.7200927734375\n",
      "train epoch 152, loss 3.1180789470672607:\n",
      "epoch: 151, loss: 30.76392364501953\n",
      "train epoch 153, loss 3.2682433128356934:\n",
      "epoch: 152, loss: 26.815385818481445\n",
      "train epoch 154, loss 3.761345624923706:\n",
      "epoch: 153, loss: 25.2867488861084\n",
      "train epoch 155, loss 4.372164249420166:\n",
      "epoch: 154, loss: 27.68364715576172\n",
      "train epoch 156, loss 2.2577152252197266:\n",
      "epoch: 155, loss: 30.60128402709961\n",
      "train epoch 157, loss 8.978458404541016:\n",
      "epoch: 156, loss: 29.828500747680664\n",
      "train epoch 158, loss 3.0426840782165527:\n",
      "epoch: 157, loss: 26.3328857421875\n",
      "train epoch 159, loss 4.048173427581787:\n",
      "epoch: 158, loss: 26.887590408325195\n",
      "train epoch 160, loss 4.1742095947265625:\n",
      "epoch: 159, loss: 26.476091384887695\n",
      "train epoch 161, loss 3.5958964824676514:\n",
      "epoch: 160, loss: 28.281763076782227\n",
      "train epoch 162, loss 8.121575355529785:\n",
      "epoch: 161, loss: 29.279382705688477\n",
      "train epoch 163, loss 8.578685760498047:\n",
      "epoch: 162, loss: 28.077653884887695\n",
      "train epoch 164, loss 1.3870387077331543:\n",
      "epoch: 163, loss: 24.722753524780273\n",
      "train epoch 165, loss 2.9286975860595703:\n",
      "epoch: 164, loss: 30.909543991088867\n",
      "train epoch 166, loss 3.2886993885040283:\n",
      "epoch: 165, loss: 27.81867027282715\n",
      "train epoch 167, loss 8.000986099243164:\n",
      "epoch: 166, loss: 32.308353424072266\n",
      "train epoch 168, loss 3.0547728538513184:\n",
      "epoch: 167, loss: 30.898847579956055\n",
      "train epoch 169, loss 4.906185150146484:\n",
      "epoch: 168, loss: 32.13847732543945\n",
      "train epoch 170, loss 4.662631034851074:\n",
      "epoch: 169, loss: 31.05506134033203\n",
      "train epoch 171, loss 1.9707386493682861:\n",
      "epoch: 170, loss: 38.689422607421875\n",
      "train epoch 172, loss 5.549195289611816:\n",
      "epoch: 171, loss: 34.29743576049805\n",
      "train epoch 173, loss 14.114486694335938:\n",
      "epoch: 172, loss: 31.630935668945312\n",
      "train epoch 174, loss 4.681138515472412:\n",
      "epoch: 173, loss: 35.99031448364258\n",
      "train epoch 175, loss 6.8860368728637695:\n",
      "epoch: 174, loss: 31.196401596069336\n",
      "train epoch 176, loss 4.055686950683594:\n",
      "epoch: 175, loss: 34.005001068115234\n",
      "train epoch 177, loss 8.976985931396484:\n",
      "epoch: 176, loss: 35.17286682128906\n",
      "train epoch 178, loss 4.647002696990967:\n",
      "epoch: 177, loss: 38.17657470703125\n",
      "train epoch 179, loss 3.028456926345825:\n",
      "epoch: 178, loss: 34.53055953979492\n",
      "train epoch 180, loss 6.214625358581543:\n",
      "epoch: 179, loss: 39.375362396240234\n",
      "train epoch 181, loss 1.5693804025650024:\n",
      "epoch: 180, loss: 32.292198181152344\n",
      "train epoch 182, loss 3.8627586364746094:\n",
      "epoch: 181, loss: 29.166358947753906\n",
      "train epoch 183, loss 6.1119303703308105:\n",
      "epoch: 182, loss: 29.109628677368164\n",
      "train epoch 184, loss 4.908267974853516:\n",
      "epoch: 183, loss: 30.699695587158203\n",
      "train epoch 185, loss 1.4258363246917725:\n",
      "epoch: 184, loss: 28.521352767944336\n",
      "train epoch 186, loss 2.3606576919555664:\n",
      "epoch: 185, loss: 33.20527267456055\n",
      "train epoch 187, loss 4.238736629486084:\n",
      "epoch: 186, loss: 33.381736755371094\n",
      "train epoch 188, loss 3.087120532989502:\n",
      "epoch: 187, loss: 33.06529235839844\n",
      "train epoch 189, loss 3.2922120094299316:\n",
      "epoch: 188, loss: 29.23107147216797\n",
      "train epoch 190, loss 6.599277496337891:\n",
      "epoch: 189, loss: 31.808927536010742\n",
      "train epoch 191, loss 6.9175848960876465:\n",
      "epoch: 190, loss: 36.105159759521484\n",
      "train epoch 192, loss 1.482187271118164:\n",
      "epoch: 191, loss: 38.793243408203125\n",
      "train epoch 193, loss 2.1247076988220215:\n",
      "epoch: 192, loss: 33.76066589355469\n",
      "train epoch 194, loss 7.811009407043457:\n",
      "epoch: 193, loss: 31.09296226501465\n",
      "train epoch 195, loss 9.19450855255127:\n",
      "epoch: 194, loss: 26.12908172607422\n",
      "train epoch 196, loss 1.984433650970459:\n",
      "epoch: 195, loss: 20.680580139160156\n",
      "train epoch 197, loss 2.158459186553955:\n",
      "epoch: 196, loss: 21.874235153198242\n",
      "train epoch 198, loss 4.3559370040893555:\n",
      "epoch: 197, loss: 28.925634384155273\n",
      "train epoch 199, loss 3.0268607139587402:\n",
      "epoch: 198, loss: 28.015853881835938\n",
      "train epoch 200, loss 5.683350563049316:\n",
      "epoch: 199, loss: 30.640275955200195\n",
      "train epoch 201, loss 4.725883483886719:\n",
      "epoch: 200, loss: 23.252483367919922\n",
      "train epoch 202, loss 5.033208847045898:\n",
      "epoch: 201, loss: 24.66039276123047\n",
      "train epoch 203, loss 3.459855556488037:\n",
      "epoch: 202, loss: 21.948331832885742\n",
      "train epoch 204, loss 3.7442197799682617:\n",
      "epoch: 203, loss: 28.126008987426758\n",
      "train epoch 205, loss 4.778299808502197:\n",
      "epoch: 204, loss: 25.638917922973633\n",
      "train epoch 206, loss 1.9420266151428223:\n",
      "epoch: 205, loss: 34.30094909667969\n",
      "train epoch 207, loss 3.8293113708496094:\n",
      "epoch: 206, loss: 28.1209716796875\n",
      "train epoch 208, loss 2.659850597381592:\n",
      "epoch: 207, loss: 28.98395538330078\n",
      "train epoch 209, loss 4.635936260223389:\n",
      "epoch: 208, loss: 27.440393447875977\n",
      "train epoch 210, loss 6.498645305633545:\n",
      "epoch: 209, loss: 29.902790069580078\n",
      "train epoch 211, loss 2.9292099475860596:\n",
      "epoch: 210, loss: 28.269859313964844\n",
      "train epoch 212, loss 3.2083237171173096:\n",
      "epoch: 211, loss: 26.945636749267578\n",
      "train epoch 213, loss 5.773397445678711:\n",
      "epoch: 212, loss: 29.615036010742188\n",
      "train epoch 214, loss 3.9550881385803223:\n",
      "epoch: 213, loss: 29.16360092163086\n",
      "train epoch 215, loss 2.307565689086914:\n",
      "epoch: 214, loss: 38.691810607910156\n",
      "train epoch 216, loss 10.03104305267334:\n",
      "epoch: 215, loss: 30.676565170288086\n",
      "train epoch 217, loss 10.136441230773926:\n",
      "epoch: 216, loss: 30.86402130126953\n",
      "train epoch 218, loss 9.472440719604492:\n",
      "epoch: 217, loss: 26.6340389251709\n",
      "train epoch 219, loss 2.0524520874023438:\n",
      "epoch: 218, loss: 21.284086227416992\n",
      "train epoch 220, loss 4.2662553787231445:\n",
      "epoch: 219, loss: 25.05539321899414\n",
      "train epoch 221, loss 1.8837929964065552:\n",
      "epoch: 220, loss: 37.841285705566406\n",
      "train epoch 222, loss 9.798343658447266:\n",
      "epoch: 221, loss: 32.47960662841797\n",
      "train epoch 223, loss 2.1576828956604004:\n",
      "epoch: 222, loss: 34.32643127441406\n",
      "train epoch 224, loss 5.296828269958496:\n",
      "epoch: 223, loss: 30.214797973632812\n",
      "train epoch 225, loss 3.0793516635894775:\n",
      "epoch: 224, loss: 32.83549499511719\n",
      "train epoch 226, loss 5.0246758460998535:\n",
      "epoch: 225, loss: 35.89651870727539\n",
      "train epoch 227, loss 8.67049503326416:\n",
      "epoch: 226, loss: 31.539838790893555\n",
      "train epoch 228, loss 3.549112319946289:\n",
      "epoch: 227, loss: 34.847877502441406\n",
      "train epoch 229, loss 4.734870910644531:\n",
      "epoch: 228, loss: 30.661659240722656\n",
      "train epoch 230, loss 2.970320224761963:\n",
      "epoch: 229, loss: 31.514162063598633\n",
      "train epoch 231, loss 4.434169292449951:\n",
      "epoch: 230, loss: 29.451391220092773\n",
      "train epoch 232, loss 3.3509387969970703:\n",
      "epoch: 231, loss: 30.827558517456055\n",
      "train epoch 233, loss 6.672388553619385:\n",
      "epoch: 232, loss: 26.59521484375\n",
      "train epoch 234, loss 7.961947441101074:\n",
      "epoch: 233, loss: 37.32587814331055\n",
      "train epoch 235, loss 2.816358804702759:\n",
      "epoch: 234, loss: 28.898778915405273\n",
      "train epoch 236, loss 6.6064558029174805:\n",
      "epoch: 235, loss: 42.543357849121094\n",
      "train epoch 237, loss 4.544662952423096:\n",
      "epoch: 236, loss: 26.607704162597656\n",
      "train epoch 238, loss 4.2185845375061035:\n",
      "epoch: 237, loss: 29.536725997924805\n",
      "train epoch 239, loss 5.154603958129883:\n",
      "epoch: 238, loss: 27.846914291381836\n",
      "train epoch 240, loss 2.798877000808716:\n",
      "epoch: 239, loss: 31.516616821289062\n",
      "train epoch 241, loss 10.479813575744629:\n",
      "epoch: 240, loss: 32.21855545043945\n",
      "train epoch 242, loss 3.2740604877471924:\n",
      "epoch: 241, loss: 36.329376220703125\n",
      "train epoch 243, loss 5.964599609375:\n",
      "epoch: 242, loss: 33.1346549987793\n",
      "train epoch 244, loss 2.6769516468048096:\n",
      "epoch: 243, loss: 32.582576751708984\n",
      "train epoch 245, loss 2.559269905090332:\n",
      "epoch: 244, loss: 31.887449264526367\n",
      "train epoch 246, loss 2.881101369857788:\n",
      "epoch: 245, loss: 29.82015609741211\n",
      "train epoch 247, loss 8.625469207763672:\n",
      "epoch: 246, loss: 30.094961166381836\n",
      "train epoch 248, loss 8.98308277130127:\n",
      "epoch: 247, loss: 25.965587615966797\n",
      "train epoch 249, loss 2.9160525798797607:\n",
      "epoch: 248, loss: 33.24689483642578\n",
      "train epoch 250, loss 3.3501904010772705:\n",
      "epoch: 249, loss: 27.897972106933594\n",
      "train epoch 251, loss 7.254242897033691:\n",
      "epoch: 250, loss: 25.071725845336914\n",
      "train epoch 252, loss 3.562138319015503:\n",
      "epoch: 251, loss: 21.3220157623291\n",
      "train epoch 253, loss 2.7880282402038574:\n",
      "epoch: 252, loss: 20.562192916870117\n",
      "train epoch 254, loss 4.167861461639404:\n",
      "epoch: 253, loss: 29.22958755493164\n",
      "train epoch 255, loss 3.9433836936950684:\n",
      "epoch: 254, loss: 32.659271240234375\n",
      "train epoch 256, loss 7.786426067352295:\n",
      "epoch: 255, loss: 32.579227447509766\n",
      "train epoch 257, loss 8.000787734985352:\n",
      "epoch: 256, loss: 31.8048038482666\n",
      "train epoch 258, loss 10.853964805603027:\n",
      "epoch: 257, loss: 31.019994735717773\n",
      "train epoch 259, loss 2.9521641731262207:\n",
      "epoch: 258, loss: 37.96965789794922\n",
      "train epoch 260, loss 2.022582530975342:\n",
      "epoch: 259, loss: 34.81059646606445\n",
      "train epoch 261, loss 4.391632080078125:\n",
      "epoch: 260, loss: 26.902904510498047\n",
      "train epoch 262, loss 7.935004234313965:\n",
      "epoch: 261, loss: 42.354427337646484\n",
      "train epoch 263, loss 5.76345682144165:\n",
      "epoch: 262, loss: 33.397987365722656\n",
      "train epoch 264, loss 6.153163433074951:\n",
      "epoch: 263, loss: 37.8354377746582\n",
      "train epoch 265, loss 2.5053887367248535:\n",
      "epoch: 264, loss: 28.481252670288086\n",
      "train epoch 266, loss 7.085855960845947:\n",
      "epoch: 265, loss: 26.500133514404297\n",
      "train epoch 267, loss 10.878276824951172:\n",
      "epoch: 266, loss: 30.89535903930664\n",
      "train epoch 268, loss 3.2585859298706055:\n",
      "epoch: 267, loss: 36.76493835449219\n",
      "train epoch 269, loss 3.3771347999572754:\n",
      "epoch: 268, loss: 30.738052368164062\n",
      "train epoch 270, loss 3.153963565826416:\n",
      "epoch: 269, loss: 28.571413040161133\n",
      "train epoch 271, loss 1.6904152631759644:\n",
      "epoch: 270, loss: 28.290691375732422\n",
      "train epoch 272, loss 7.087734222412109:\n",
      "epoch: 271, loss: 24.5689754486084\n",
      "train epoch 273, loss 1.6098285913467407:\n",
      "epoch: 272, loss: 22.055381774902344\n",
      "train epoch 274, loss 1.1515843868255615:\n",
      "epoch: 273, loss: 20.416748046875\n",
      "train epoch 275, loss 3.755896806716919:\n",
      "epoch: 274, loss: 27.973705291748047\n",
      "train epoch 276, loss 4.2083210945129395:\n",
      "epoch: 275, loss: 24.573091506958008\n",
      "train epoch 277, loss 4.997256278991699:\n",
      "epoch: 276, loss: 29.72259521484375\n",
      "train epoch 278, loss 7.0225114822387695:\n",
      "epoch: 277, loss: 28.120159149169922\n",
      "train epoch 279, loss 2.063225030899048:\n",
      "epoch: 278, loss: 35.01945495605469\n",
      "train epoch 280, loss 2.4777276515960693:\n",
      "epoch: 279, loss: 29.257566452026367\n",
      "train epoch 281, loss 3.2637393474578857:\n",
      "epoch: 280, loss: 29.739233016967773\n",
      "train epoch 282, loss 2.621042490005493:\n",
      "epoch: 281, loss: 31.18415069580078\n",
      "train epoch 283, loss 2.6093878746032715:\n",
      "epoch: 282, loss: 30.20763397216797\n",
      "train epoch 284, loss 1.1590849161148071:\n",
      "epoch: 283, loss: 26.820903778076172\n",
      "train epoch 285, loss 2.6875510215759277:\n",
      "epoch: 284, loss: 30.79928207397461\n",
      "train epoch 286, loss 3.5662145614624023:\n",
      "epoch: 285, loss: 27.005529403686523\n",
      "train epoch 287, loss 2.4047398567199707:\n",
      "epoch: 286, loss: 28.825511932373047\n",
      "train epoch 288, loss 6.5612688064575195:\n",
      "epoch: 287, loss: 26.116552352905273\n",
      "train epoch 289, loss 1.177711009979248:\n",
      "epoch: 288, loss: 26.033226013183594\n",
      "train epoch 290, loss 2.8408572673797607:\n",
      "epoch: 289, loss: 26.117107391357422\n",
      "train epoch 291, loss 4.703497409820557:\n",
      "epoch: 290, loss: 31.319438934326172\n",
      "train epoch 292, loss 5.308216571807861:\n",
      "epoch: 291, loss: 27.435836791992188\n",
      "train epoch 293, loss 4.8090715408325195:\n",
      "epoch: 292, loss: 24.8638973236084\n",
      "train epoch 294, loss 2.490159034729004:\n",
      "epoch: 293, loss: 24.386310577392578\n",
      "train epoch 295, loss 1.9706568717956543:\n",
      "epoch: 294, loss: 30.951631546020508\n",
      "train epoch 296, loss 4.663753509521484:\n",
      "epoch: 295, loss: 31.465070724487305\n",
      "train epoch 297, loss 1.2060930728912354:\n",
      "epoch: 296, loss: 29.183387756347656\n",
      "train epoch 298, loss 2.403256893157959:\n",
      "epoch: 297, loss: 28.813575744628906\n",
      "train epoch 299, loss 4.150547981262207:\n",
      "epoch: 298, loss: 23.953754425048828\n",
      "train epoch 300, loss 3.648977041244507:\n",
      "epoch: 299, loss: 35.591835021972656\n",
      "train epoch 301, loss 4.203701019287109:\n",
      "epoch: 300, loss: 26.026050567626953\n",
      "train epoch 302, loss 4.548041343688965:\n",
      "epoch: 301, loss: 30.743986129760742\n",
      "train epoch 303, loss 4.693612575531006:\n",
      "epoch: 302, loss: 27.54303550720215\n",
      "train epoch 304, loss 4.058743953704834:\n",
      "epoch: 303, loss: 24.521947860717773\n",
      "train epoch 305, loss 2.032846689224243:\n",
      "epoch: 304, loss: 29.911376953125\n",
      "train epoch 306, loss 5.670267581939697:\n",
      "epoch: 305, loss: 29.161638259887695\n",
      "train epoch 307, loss 5.764843463897705:\n",
      "epoch: 306, loss: 27.59188461303711\n",
      "train epoch 308, loss 3.0015525817871094:\n",
      "epoch: 307, loss: 28.3243465423584\n",
      "train epoch 309, loss 3.4466395378112793:\n",
      "epoch: 308, loss: 24.05942153930664\n",
      "train epoch 310, loss 3.584953546524048:\n",
      "epoch: 309, loss: 26.256216049194336\n",
      "train epoch 311, loss 7.073158264160156:\n",
      "epoch: 310, loss: 29.385826110839844\n",
      "train epoch 312, loss 2.098113536834717:\n",
      "epoch: 311, loss: 28.43239974975586\n",
      "train epoch 313, loss 4.051268577575684:\n",
      "epoch: 312, loss: 26.354248046875\n",
      "train epoch 314, loss 1.4243640899658203:\n",
      "epoch: 313, loss: 21.50899887084961\n",
      "train epoch 315, loss 2.143927574157715:\n",
      "epoch: 314, loss: 23.023784637451172\n",
      "train epoch 316, loss 2.8397183418273926:\n",
      "epoch: 315, loss: 33.251441955566406\n",
      "train epoch 317, loss 2.697101593017578:\n",
      "epoch: 316, loss: 27.378164291381836\n",
      "train epoch 318, loss 3.2707862854003906:\n",
      "epoch: 317, loss: 28.758100509643555\n",
      "train epoch 319, loss 2.1396126747131348:\n",
      "epoch: 318, loss: 26.4152889251709\n",
      "train epoch 320, loss 1.956430196762085:\n",
      "epoch: 319, loss: 27.210407257080078\n",
      "train epoch 321, loss 5.765711784362793:\n",
      "epoch: 320, loss: 35.4617805480957\n",
      "train epoch 322, loss 1.3704856634140015:\n",
      "epoch: 321, loss: 27.448518753051758\n",
      "train epoch 323, loss 4.832571983337402:\n",
      "epoch: 322, loss: 29.16327667236328\n",
      "train epoch 324, loss 2.7185966968536377:\n",
      "epoch: 323, loss: 29.968605041503906\n",
      "train epoch 325, loss 1.8938570022583008:\n",
      "epoch: 324, loss: 27.266185760498047\n",
      "train epoch 326, loss 2.4379172325134277:\n",
      "epoch: 325, loss: 29.585535049438477\n",
      "train epoch 327, loss 2.2123403549194336:\n",
      "epoch: 326, loss: 28.300373077392578\n",
      "train epoch 328, loss 2.007179021835327:\n",
      "epoch: 327, loss: 30.764690399169922\n",
      "train epoch 329, loss 9.7906494140625:\n",
      "epoch: 328, loss: 29.4276065826416\n",
      "train epoch 330, loss 3.4810221195220947:\n",
      "epoch: 329, loss: 29.224414825439453\n",
      "train epoch 331, loss 0.9879418611526489:\n",
      "epoch: 330, loss: 27.560985565185547\n",
      "train epoch 332, loss 2.3239147663116455:\n",
      "epoch: 331, loss: 26.765321731567383\n",
      "train epoch 333, loss 4.964237213134766:\n",
      "epoch: 332, loss: 25.930164337158203\n",
      "train epoch 334, loss 2.4537582397460938:\n",
      "epoch: 333, loss: 28.486309051513672\n",
      "train epoch 335, loss 3.160579204559326:\n",
      "epoch: 334, loss: 28.9276123046875\n",
      "train epoch 336, loss 1.3758183717727661:\n",
      "epoch: 335, loss: 31.559457778930664\n",
      "train epoch 337, loss 3.1594200134277344:\n",
      "epoch: 336, loss: 26.87262725830078\n",
      "train epoch 338, loss 1.0657402276992798:\n",
      "epoch: 337, loss: 33.3597297668457\n",
      "train epoch 339, loss 7.627882480621338:\n",
      "epoch: 338, loss: 29.672046661376953\n",
      "train epoch 340, loss 2.153109312057495:\n",
      "epoch: 339, loss: 25.283594131469727\n",
      "train epoch 341, loss 3.8316357135772705:\n",
      "epoch: 340, loss: 26.270530700683594\n",
      "train epoch 342, loss 3.9056904315948486:\n",
      "epoch: 341, loss: 25.411354064941406\n",
      "train epoch 343, loss 1.8824620246887207:\n",
      "epoch: 342, loss: 24.47174835205078\n",
      "train epoch 344, loss 1.0044740438461304:\n",
      "epoch: 343, loss: 29.27444839477539\n",
      "train epoch 345, loss 5.359155178070068:\n",
      "epoch: 344, loss: 29.142831802368164\n",
      "train epoch 346, loss 1.242488145828247:\n",
      "epoch: 345, loss: 32.087398529052734\n",
      "train epoch 347, loss 2.3642520904541016:\n",
      "epoch: 346, loss: 27.262065887451172\n",
      "train epoch 348, loss 2.6576907634735107:\n",
      "epoch: 347, loss: 26.934345245361328\n",
      "train epoch 349, loss 1.9086285829544067:\n",
      "epoch: 348, loss: 26.736583709716797\n",
      "train epoch 350, loss 4.889251708984375:\n",
      "epoch: 349, loss: 23.786924362182617\n",
      "train epoch 351, loss 4.280916213989258:\n",
      "epoch: 350, loss: 25.501670837402344\n",
      "train epoch 352, loss 1.2046526670455933:\n",
      "epoch: 351, loss: 24.642961502075195\n",
      "train epoch 353, loss 3.003164768218994:\n",
      "epoch: 352, loss: 29.652029037475586\n",
      "train epoch 354, loss 2.7969157695770264:\n",
      "epoch: 353, loss: 25.928356170654297\n",
      "train epoch 355, loss 1.6597623825073242:\n",
      "epoch: 354, loss: 27.569337844848633\n",
      "train epoch 356, loss 1.5302799940109253:\n",
      "epoch: 355, loss: 28.78557777404785\n",
      "train epoch 357, loss 1.4684041738510132:\n",
      "epoch: 356, loss: 29.017593383789062\n",
      "train epoch 358, loss 4.463184833526611:\n",
      "epoch: 357, loss: 25.8229923248291\n",
      "train epoch 359, loss 3.198594093322754:\n",
      "epoch: 358, loss: 23.94011878967285\n",
      "train epoch 360, loss 2.2098217010498047:\n",
      "epoch: 359, loss: 25.21564483642578\n",
      "train epoch 361, loss 3.384678363800049:\n",
      "epoch: 360, loss: 30.262636184692383\n",
      "train epoch 362, loss 2.0187618732452393:\n",
      "epoch: 361, loss: 28.851890563964844\n",
      "train epoch 363, loss 2.7212467193603516:\n",
      "epoch: 362, loss: 26.991411209106445\n",
      "train epoch 364, loss 7.011964797973633:\n",
      "epoch: 363, loss: 33.387237548828125\n",
      "train epoch 365, loss 3.719383716583252:\n",
      "epoch: 364, loss: 34.72513198852539\n",
      "train epoch 366, loss 4.273929119110107:\n",
      "epoch: 365, loss: 32.44276809692383\n",
      "train epoch 367, loss 1.6653622388839722:\n",
      "epoch: 366, loss: 25.644014358520508\n",
      "train epoch 368, loss 4.515445709228516:\n",
      "epoch: 367, loss: 28.287216186523438\n",
      "train epoch 369, loss 2.867183208465576:\n",
      "epoch: 368, loss: 27.780946731567383\n",
      "train epoch 370, loss 1.5732442140579224:\n",
      "epoch: 369, loss: 28.033464431762695\n",
      "train epoch 371, loss 3.3081395626068115:\n",
      "epoch: 370, loss: 27.267637252807617\n",
      "train epoch 372, loss 4.128438949584961:\n",
      "epoch: 371, loss: 24.865140914916992\n",
      "train epoch 373, loss 3.5001327991485596:\n",
      "epoch: 372, loss: 24.853288650512695\n",
      "train epoch 374, loss 1.5392450094223022:\n",
      "epoch: 373, loss: 27.859657287597656\n",
      "train epoch 375, loss 5.101142883300781:\n",
      "epoch: 374, loss: 27.33706283569336\n",
      "train epoch 376, loss 4.1566948890686035:\n",
      "epoch: 375, loss: 25.850513458251953\n",
      "train epoch 377, loss 2.4858815670013428:\n",
      "epoch: 376, loss: 28.61602210998535\n",
      "train epoch 378, loss 8.75374698638916:\n",
      "epoch: 377, loss: 28.73601722717285\n",
      "train epoch 379, loss 3.7678728103637695:\n",
      "epoch: 378, loss: 26.702939987182617\n",
      "train epoch 380, loss 2.585660219192505:\n",
      "epoch: 379, loss: 32.48872375488281\n",
      "train epoch 381, loss 7.124744415283203:\n",
      "epoch: 380, loss: 26.15935516357422\n",
      "train epoch 382, loss 3.884152412414551:\n",
      "epoch: 381, loss: 26.350778579711914\n",
      "train epoch 383, loss 1.7326147556304932:\n",
      "epoch: 382, loss: 28.951641082763672\n",
      "train epoch 384, loss 2.4637465476989746:\n",
      "epoch: 383, loss: 27.853395462036133\n",
      "train epoch 385, loss 2.5226452350616455:\n",
      "epoch: 384, loss: 30.64940643310547\n",
      "train epoch 386, loss 4.0722270011901855:\n",
      "epoch: 385, loss: 25.883451461791992\n",
      "train epoch 387, loss 4.58440637588501:\n",
      "epoch: 386, loss: 28.573362350463867\n",
      "train epoch 388, loss 6.378373622894287:\n",
      "epoch: 387, loss: 25.47927474975586\n",
      "train epoch 389, loss 2.8097305297851562:\n",
      "epoch: 388, loss: 29.09676742553711\n",
      "train epoch 390, loss 3.260939359664917:\n",
      "epoch: 389, loss: 26.234130859375\n",
      "train epoch 391, loss 2.935864210128784:\n",
      "epoch: 390, loss: 26.69317626953125\n",
      "train epoch 392, loss 1.6104117631912231:\n",
      "epoch: 391, loss: 32.63313674926758\n",
      "train epoch 393, loss 2.2659826278686523:\n",
      "epoch: 392, loss: 25.97760009765625\n",
      "train epoch 394, loss 2.1459054946899414:\n",
      "epoch: 393, loss: 26.447978973388672\n",
      "train epoch 395, loss 2.2997193336486816:\n",
      "epoch: 394, loss: 29.40245819091797\n",
      "train epoch 396, loss 1.803005576133728:\n",
      "epoch: 395, loss: 25.892683029174805\n",
      "train epoch 397, loss 3.735621690750122:\n",
      "epoch: 396, loss: 29.784446716308594\n",
      "train epoch 398, loss 4.809723854064941:\n",
      "epoch: 397, loss: 27.789400100708008\n",
      "train epoch 399, loss 1.705622911453247:\n",
      "epoch: 398, loss: 26.11850357055664\n",
      "train epoch 400, loss 2.1858420372009277:\n",
      "epoch: 399, loss: 29.789634704589844\n",
      "train epoch 401, loss 3.077277660369873:\n",
      "epoch: 400, loss: 26.923521041870117\n",
      "train epoch 402, loss 4.545826435089111:\n",
      "epoch: 401, loss: 26.739885330200195\n",
      "train epoch 403, loss 1.247360348701477:\n",
      "epoch: 402, loss: 27.9335994720459\n",
      "train epoch 404, loss 4.820279598236084:\n",
      "epoch: 403, loss: 31.369007110595703\n",
      "train epoch 405, loss 9.166247367858887:\n",
      "epoch: 404, loss: 26.216360092163086\n",
      "train epoch 406, loss 1.826132893562317:\n",
      "epoch: 405, loss: 28.83370590209961\n",
      "train epoch 407, loss 1.5953376293182373:\n",
      "epoch: 406, loss: 30.992475509643555\n",
      "train epoch 408, loss 6.537984371185303:\n",
      "epoch: 407, loss: 25.800825119018555\n",
      "train epoch 409, loss 1.9732770919799805:\n",
      "epoch: 408, loss: 24.153121948242188\n",
      "train epoch 410, loss 1.6657286882400513:\n",
      "epoch: 409, loss: 26.46068572998047\n",
      "train epoch 411, loss 1.7799594402313232:\n",
      "epoch: 410, loss: 25.309730529785156\n",
      "train epoch 412, loss 2.2980217933654785:\n",
      "epoch: 411, loss: 26.7313289642334\n",
      "train epoch 413, loss 5.886496067047119:\n",
      "epoch: 412, loss: 27.980207443237305\n",
      "train epoch 414, loss 2.529315710067749:\n",
      "epoch: 413, loss: 24.820030212402344\n",
      "train epoch 415, loss 1.3904340267181396:\n",
      "epoch: 414, loss: 28.21887969970703\n",
      "train epoch 416, loss 1.701092004776001:\n",
      "epoch: 415, loss: 24.2116641998291\n",
      "train epoch 417, loss 5.472082138061523:\n",
      "epoch: 416, loss: 24.829627990722656\n",
      "train epoch 418, loss 1.9230303764343262:\n",
      "epoch: 417, loss: 26.20317840576172\n",
      "train epoch 419, loss 1.190936803817749:\n",
      "epoch: 418, loss: 26.81473159790039\n",
      "train epoch 420, loss 1.5924270153045654:\n",
      "epoch: 419, loss: 34.22454833984375\n",
      "train epoch 421, loss 0.9392973780632019:\n",
      "epoch: 420, loss: 27.55710220336914\n",
      "train epoch 422, loss 3.0265471935272217:\n",
      "epoch: 421, loss: 26.100629806518555\n",
      "train epoch 423, loss 1.7651395797729492:\n",
      "epoch: 422, loss: 29.82733917236328\n",
      "train epoch 424, loss 5.404065132141113:\n",
      "epoch: 423, loss: 29.382112503051758\n",
      "train epoch 425, loss 4.611258506774902:\n",
      "epoch: 424, loss: 35.068511962890625\n",
      "train epoch 426, loss 1.7527631521224976:\n",
      "epoch: 425, loss: 26.056364059448242\n",
      "train epoch 427, loss 4.095698356628418:\n",
      "epoch: 426, loss: 31.023056030273438\n",
      "train epoch 428, loss 3.1385507583618164:\n",
      "epoch: 427, loss: 29.299596786499023\n",
      "train epoch 429, loss 5.84765625:\n",
      "epoch: 428, loss: 27.592082977294922\n",
      "train epoch 430, loss 1.4610319137573242:\n",
      "epoch: 429, loss: 29.9202938079834\n",
      "train epoch 431, loss 2.411062717437744:\n",
      "epoch: 430, loss: 28.113149642944336\n",
      "train epoch 432, loss 4.68546199798584:\n",
      "epoch: 431, loss: 31.533849716186523\n",
      "train epoch 433, loss 4.420527458190918:\n",
      "epoch: 432, loss: 27.6530818939209\n",
      "train epoch 434, loss 0.9311054944992065:\n",
      "epoch: 433, loss: 32.04186248779297\n",
      "train epoch 435, loss 1.9831205606460571:\n",
      "epoch: 434, loss: 33.38825607299805\n",
      "train epoch 436, loss 3.449427366256714:\n",
      "epoch: 435, loss: 26.461685180664062\n",
      "train epoch 437, loss 2.041416883468628:\n",
      "epoch: 436, loss: 28.606184005737305\n",
      "train epoch 438, loss 3.3687939643859863:\n",
      "epoch: 437, loss: 32.07063293457031\n",
      "train epoch 439, loss 2.9864602088928223:\n",
      "epoch: 438, loss: 25.097105026245117\n",
      "train epoch 440, loss 4.330544471740723:\n",
      "epoch: 439, loss: 25.949378967285156\n",
      "train epoch 441, loss 3.4146769046783447:\n",
      "epoch: 440, loss: 30.634199142456055\n",
      "train epoch 442, loss 5.286862850189209:\n",
      "epoch: 441, loss: 26.227035522460938\n",
      "train epoch 443, loss 5.927122116088867:\n",
      "epoch: 442, loss: 31.761274337768555\n",
      "train epoch 444, loss 9.524738311767578:\n",
      "epoch: 443, loss: 30.08222007751465\n",
      "train epoch 445, loss 4.5218586921691895:\n",
      "epoch: 444, loss: 27.390417098999023\n",
      "train epoch 446, loss 4.701563835144043:\n",
      "epoch: 445, loss: 34.035987854003906\n",
      "train epoch 447, loss 1.9811044931411743:\n",
      "epoch: 446, loss: 31.065868377685547\n",
      "train epoch 448, loss 1.9244768619537354:\n",
      "epoch: 447, loss: 28.608823776245117\n",
      "train epoch 449, loss 4.862306594848633:\n",
      "epoch: 448, loss: 26.582481384277344\n",
      "train epoch 450, loss 3.9761693477630615:\n",
      "epoch: 449, loss: 29.6156005859375\n",
      "train epoch 451, loss 2.750807285308838:\n",
      "epoch: 450, loss: 26.944503784179688\n",
      "train epoch 452, loss 4.582448959350586:\n",
      "epoch: 451, loss: 27.73733139038086\n",
      "train epoch 453, loss 2.9649572372436523:\n",
      "epoch: 452, loss: 27.361190795898438\n",
      "train epoch 454, loss 1.823434829711914:\n",
      "epoch: 453, loss: 26.978235244750977\n",
      "train epoch 455, loss 2.4077770709991455:\n",
      "epoch: 454, loss: 29.826156616210938\n",
      "train epoch 456, loss 3.581913948059082:\n",
      "epoch: 455, loss: 25.498310089111328\n",
      "train epoch 457, loss 5.26367712020874:\n",
      "epoch: 456, loss: 24.144441604614258\n",
      "train epoch 458, loss 3.488471746444702:\n",
      "epoch: 457, loss: 28.334714889526367\n",
      "train epoch 459, loss 1.4909573793411255:\n",
      "epoch: 458, loss: 27.854501724243164\n",
      "train epoch 460, loss 3.9166054725646973:\n",
      "epoch: 459, loss: 28.41827964782715\n",
      "train epoch 461, loss 4.250107765197754:\n",
      "epoch: 460, loss: 27.926801681518555\n",
      "train epoch 462, loss 7.310176849365234:\n",
      "epoch: 461, loss: 30.929588317871094\n",
      "train epoch 463, loss 5.3898773193359375:\n",
      "epoch: 462, loss: 41.490299224853516\n",
      "train epoch 464, loss 3.8662571907043457:\n",
      "epoch: 463, loss: 31.19753646850586\n",
      "train epoch 465, loss 6.121920585632324:\n",
      "epoch: 464, loss: 25.496074676513672\n",
      "train epoch 466, loss 4.842639446258545:\n",
      "epoch: 465, loss: 32.67136764526367\n",
      "train epoch 467, loss 9.658409118652344:\n",
      "epoch: 466, loss: 29.281435012817383\n",
      "train epoch 468, loss 2.5180952548980713:\n",
      "epoch: 467, loss: 27.962575912475586\n",
      "train epoch 469, loss 1.6315594911575317:\n",
      "epoch: 468, loss: 29.42679214477539\n",
      "train epoch 470, loss 3.1623034477233887:\n",
      "epoch: 469, loss: 28.339492797851562\n",
      "train epoch 471, loss 2.650176525115967:\n",
      "epoch: 470, loss: 29.910463333129883\n",
      "train epoch 472, loss 2.0845820903778076:\n",
      "epoch: 471, loss: 28.194351196289062\n",
      "train epoch 473, loss 4.526599407196045:\n",
      "epoch: 472, loss: 24.875286102294922\n",
      "train epoch 474, loss 2.227628231048584:\n",
      "epoch: 473, loss: 28.7369327545166\n",
      "train epoch 475, loss 1.458639144897461:\n",
      "epoch: 474, loss: 25.7801570892334\n",
      "train epoch 476, loss 1.9635220766067505:\n",
      "epoch: 475, loss: 27.30701446533203\n",
      "train epoch 477, loss 2.421786069869995:\n",
      "epoch: 476, loss: 28.09282684326172\n",
      "train epoch 478, loss 2.313270092010498:\n",
      "epoch: 477, loss: 27.061983108520508\n",
      "train epoch 479, loss 2.056398391723633:\n",
      "epoch: 478, loss: 26.873228073120117\n",
      "train epoch 480, loss 3.7591686248779297:\n",
      "epoch: 479, loss: 27.492631912231445\n",
      "train epoch 481, loss 2.0837059020996094:\n",
      "epoch: 480, loss: 27.3897762298584\n",
      "train epoch 482, loss 2.230391025543213:\n",
      "epoch: 481, loss: 27.1329345703125\n",
      "train epoch 483, loss 1.5972185134887695:\n",
      "epoch: 482, loss: 24.76806640625\n",
      "train epoch 484, loss 6.450931549072266:\n",
      "epoch: 483, loss: 28.033613204956055\n",
      "train epoch 485, loss 2.9493355751037598:\n",
      "epoch: 484, loss: 27.566791534423828\n",
      "train epoch 486, loss 2.1550817489624023:\n",
      "epoch: 485, loss: 24.953327178955078\n",
      "train epoch 487, loss 1.4622281789779663:\n",
      "epoch: 486, loss: 33.015811920166016\n",
      "train epoch 488, loss 7.702920436859131:\n",
      "epoch: 487, loss: 28.852283477783203\n",
      "train epoch 489, loss 3.152757406234741:\n",
      "epoch: 488, loss: 27.816810607910156\n",
      "train epoch 490, loss 2.111560821533203:\n",
      "epoch: 489, loss: 32.945804595947266\n",
      "train epoch 491, loss 2.7281363010406494:\n",
      "epoch: 490, loss: 26.889528274536133\n",
      "train epoch 492, loss 1.510195016860962:\n",
      "epoch: 491, loss: 34.978782653808594\n",
      "train epoch 493, loss 4.056581974029541:\n",
      "epoch: 492, loss: 27.016328811645508\n",
      "train epoch 494, loss 2.102879524230957:\n",
      "epoch: 493, loss: 28.13064193725586\n",
      "train epoch 495, loss 2.971776008605957:\n",
      "epoch: 494, loss: 28.06696319580078\n",
      "train epoch 496, loss 2.3556482791900635:\n",
      "epoch: 495, loss: 26.2779598236084\n",
      "train epoch 497, loss 1.8621246814727783:\n",
      "epoch: 496, loss: 28.12811279296875\n",
      "train epoch 498, loss 5.17142391204834:\n",
      "epoch: 497, loss: 23.84170150756836\n",
      "train epoch 499, loss 2.034102439880371:\n",
      "epoch: 498, loss: 24.173139572143555\n",
      "train epoch 500, loss 2.823922634124756:\n",
      "epoch: 499, loss: 33.299137115478516\n"
     ]
    }
   ],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "feature_number = 13\n",
    "out_prediction = 1\n",
    "learning_rate = 0.01\n",
    "epochs = 500\n",
    "\n",
    "class Net(torch.nn.Module):\n",
    "    def __init__(self, n_feature, n_output, n_neuron1, n_neuron2, n_layer):\n",
    "        super(Net, self).__init__()\n",
    "        self.n_feature = n_feature\n",
    "        self.n_output = n_output\n",
    "        self.n_neuron1 = n_neuron1\n",
    "        self.n_neuron2 = n_neuron2\n",
    "        self.n_layer = n_layer\n",
    "        self.input_layer = torch.nn.Linear(self.n_feature, self.n_neuron1)\n",
    "        self.hidden1 = torch.nn.Linear(self.n_neuron1, self.n_neuron2)\n",
    "        self.hidden2 = torch.nn.Linear(self.n_neuron2, self.n_neuron2)\n",
    "        self.predict = torch.nn.Linear(self.n_neuron2, self.n_output)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.input_layer(x)\n",
    "        out = torch.relu(out)\n",
    "        out = self.hidden1(out)\n",
    "        out = torch.relu(out)\n",
    "        for i in range(self.n_layer):\n",
    "            out = self.hidden2(out)\n",
    "            out = torch.relu(out)\n",
    "        out = self.predict(out)\n",
    "        return out\n",
    "\n",
    "# class Net(torch.nn.Module):\n",
    "#     def __init__(self):\n",
    "#         super(Net, self).__init__()\n",
    "#         # First hidden layer\n",
    "#         self.h1 = torch.nn.Linear(in_features = 13, out_features=30, bias=True)\n",
    "#         self.a1 = torch.nn.ReLU()\n",
    "#         # Second hidden layer\n",
    "#         self.h2 = torch.nn.Linear(in_features=30, out_features=10)\n",
    "#         self.a2 = torch.nn.ReLU()\n",
    "#         # regression predict layer\n",
    "#         self.regression = torch.nn.Linear(in_features=10, out_features=1)\n",
    "        \n",
    "#     def forward(self, x):\n",
    "#         x = self.h1(x)\n",
    "#         x = self.a1(x)\n",
    "#         x = self.h2(x)\n",
    "#         x = self.a2(x)\n",
    "#         output = self.regression(x)\n",
    "#         return output\n",
    "\n",
    "# net = Net()\n",
    "net = Net(n_feature=feature_number, n_output=out_prediction, n_neuron1=30, n_neuron2=30, n_layer=2)\n",
    "optimizer = optim.Adam(net.parameters(), lr=learning_rate)\n",
    "# optimizer = optim.SGD(net.parameters(), lr=0.001)\n",
    "criteon = torch.nn.MSELoss()\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    net.train()\n",
    "    for batch_idx, (data, target) in enumerate(train_data):\n",
    "        # logits = net.forward(data)\n",
    "        logits = net(data).flatten()\n",
    "        loss = criteon(logits, target)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    print(\"train epoch %d, loss %s:\" % (epoch + 1, loss.item()))\n",
    "\n",
    "    logit = []\n",
    "    target = []\n",
    "    net.eval()\n",
    "    for data, targets in validation:\n",
    "        logits = net.forward(data).detach().numpy()\n",
    "        targets = targets.detach().numpy()\n",
    "        # print(logits, targets)\n",
    "        target.append(float(targets))\n",
    "        logit.append(logits[0])\n",
    "    # print(logit, target)\n",
    "    # torch.tensor(logit)\n",
    "    # torch.tensor(target)\n",
    "    average_loss = criteon(torch.tensor(logit), torch.tensor(target))\n",
    "    print('epoch: {}, loss: {}'.format(epoch, average_loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
